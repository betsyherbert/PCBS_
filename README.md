# PCBS Project: Multilayer Perceptron

For this project, I wanted to explore the architecture of simple perceptrons, both single and multi-layer. I am fascinated with the field of neural networks both in its current and previous incarnations, and have wanted for a long time to explore their background and theory in more detail. Since I am an extreme beginner in programming, I limited myself just to studying the simplest, foundational neural network architectures possible: the perceptron, and subsequently a multilayer perceptron with just one hidden layer. These form the fundamental building blocks of all current-day neural-network based models, and so I wanted to spend a long time working "from the ground up" and really trying get to grips with the computations involved. This project is a result of that. It is an attempt to describe in my own words, via examples, the fundamental theory behind perceptrons and MLPs, and subsequently code from scratch a simple MLP architecture in Python capable of solving a 'canonical' example of a non-linearly separable problem, the XOR problem.

The write-up occurs in six stages. First, I introduce the __XOR problem,__ and explain why it is non-linearly separable. I then detail the nature of __perceptrons__ and describe the intuition as to why they are only capable of forming linear decision boundaries. I then implemennt a simple __perceptron algorithm__ capable of performing linear classification on a simple dataset. Then, I introduce __multilayer perceptrons,__ and explore how it is that they can overcome this limitation. I then formalise in some detail the two major computational stages of MLPs, __forward propagation__ and __backpropagation__. Finally, I implement an __MLP architecture__ with one hidden layer, trained on the inputs and outputs of the XOR problem. We see that, after enough training iterations, it is capable of succesfully classifying the inputs into the correct outputs.

There are two points to note. Firstly, it was my original ambition to code the network using an object-oriented approach. This would be a much more generalisable method of implementation, and would allow to explore the network a little more interactively. However, after many attempts and alternative implementations, I found that I was simply not advanced enough in programming to be fully comfortable with this approach. As such, I have coded the networks using a very basic, step-wise notation; doubtless it is not "good" programming, but it allowed me to see most clearly what I was doing.

Secondly, I also wished to create other network architectures capable of performing more complex classification problems on larger datasets. I experimented with increasing the dimensionality of the training data and the number of output classes, and implementing a batch-based approach using stochastic gradient descent. I also tried using existing datasets such as the MNIST dataset and those for wheat seed classification. While I was able to get some of these architectures to function, I eventually concluded that to fully incorporate them into this short write-up would be outside the scope of this project. However, I plan to continue these explorations in my spare time, and eventually would like to move onto mastering existing code libraries. 

The write-up can be found in the Jupyter Notebook entitled "__PCBS Project Write-Up - Multilayer Perceptron__". I have kept some of my other files for the sake of not losing too much proof-of-work, but do note that these are not intended to form part of the final project.
