# PCBS Project: Multilayer Perceptron

For this PCBS project, I wanted to explore the architecture of a simple multilayer perceptron. I am fascinated with the field of neural networks both in its current and previous incarnations, and have wanted for a long time to explore more deeply their background and theory. Since I am an extreme beginner in programming, I limited myself just to studying the simplest, foundational neural network architectures possible: the perceptron, and subsequently a multilayer perceptron with one hidden layer. These form the fundamental building blocks of all current-day neural-network based models, and so I spent a long time working "from the ground up" and really trying get to grips with the computations involved. This project is a result of that. It is an attempt to describe in my own words, via examples, the fundamental theory behind perceptrons and MLPs, and subsequently code from scratch a simple MLP architecture in Python capable of solving a 'canonical' example of a non-linearly separable problem, the XOR problem.

The write-up occurs in _ stages. First, I introduce the __XOR problem,__ and explain why it is non-linearly separable. I then detail the nature of __perceptrons__ and form an intuition as to why they are only capable of forming linear decision boundaries. Then, I introduce __multilayer perceptrons,__ and explore how it is that they can overcome this limitation. I then formalise in some detail the two major computational stages of MLPs,__forward propagation__ and __backpropagation__. Finally, I translate this theoretical background into a simple Python script implementing an __MLP architecture__ with one hidden layer, trained on the inputs and outputs of the XOR problem. After enough training iterations, we see that it can succesfuly classify the inputs into the correct outputs.


The aim of this project will be to code a variety of neural network algorithms and architectures from scratch - instead of using a predefined library - in order to gain a comprehensive understanding of what is happening 'under the hood'.

Firstly, I will create a simple multilayer perceptron capable of solving a basic non-linearly separable classification problem, the XOR problem. Since there will be only 1 hidden layer, I will code the weights, biases and errors for each layer explicitly, to get an expanded overview of the algorithm's functioning. I will also create visualisations of the separation we are trying to achieve and of the reduction in error over time.

Next, I will move onto a larger classification problem, using a toy dataset with more training data but still two output categories. For this architecture I code each step as universal functions to allow hyperparameters such as the number of hidden layers to be modified freely. I will use for-loops to compress the computational steps into shorter lines of code, and begin to streamline the process a little. Here I will be able to explore the effect of modifying different hyperparameters on the accuracy and rate of convergence of the network. 

Finally, I will use a real dataset with seven training variables and three output categories, classifying species of wheat seed (https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv). Here I will try to make my  code implementation even more flexible and generalisable, to allow it to be used on any dataset with any number of variables and output categories. I will code a pipeline to split the dataset into epochs and train using online stochastic gradient descent to achieve a reasonable training time for the network. (Possibility to also try object-oriented approach ?)


