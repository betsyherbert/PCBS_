{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "A __neural network__ is a method in the field of machine learning used to build predictive models to help to solve complex tasks. By exposing the network to a large amount of training data, we can train it to approximate any arbitrary input-output function. \n",
    "\n",
    "The basic building blocks are functional modules known as nodes or \"neurons\", which take in an input, apply a mathematical operation, and produce an output. The network consists of many layers of these neurons; the outputs of one layer are fed as inputs to the next. By stacking the layers together, the input-output mapping can become arbitrarily complex. The aim is to refine the network to map inputs to the desired outputs. This is done by adding a weight and a bias to each node, and gradually adjusting these to get closer and closer to a desired output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The XOR problem\n",
    "\n",
    "The idea behind the XOR, or \"exclusive or\" problem, is to use a neural network to predict the outputs of an XOR logic gate given two binary inputs. An XOR function should return a true value if the two inputs are not equal, and a false value if they are equal. All possible inputs and predicted outputs are shown in _Figure 1_ :\n",
    "\n",
    "\n",
    "![Figure 1](img/XOR.png \"XOR truth table\")\n",
    "\n",
    "We can define these inputs and outputs using the following numpy arrays:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input array (x) and output array (y)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([[0, 0], \n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]]) \n",
    "\n",
    "y = np.array([[0,1,1,0]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If we colour the inputs (A and B) according to their classification, we see that the decision boundary for correct classification is not linear. That is, we cannot split the two categories with a single straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.grid()\n",
    "\n",
    "for i in range(len(x)):\n",
    "    c = 'r'\n",
    "    if y[i] == 0:\n",
    "        c = 'b'\n",
    "    plt.scatter([x[i][0]], [x[i][1]], c=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "The simplest neural network is known as a _perceptron,_ invented by Rosenblatt in 1958. They have just one input and one output layer. The activation function commonly used is a _step function,_ which converts the result to either a 1 or a 0. The network hence performs binary classification.\n",
    "\n",
    "We can visualise the architecture of a perceptron in _Figure 2_ :\n",
    "\n",
    "![Figure 1](img/perceptron.png \"Perceptron\")\n",
    "\n",
    "Although they once appeared promising, perceptrons are limited in their function. They are only capable of solving problems which are linearly separable. Hence, they cannot solve the XOR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons \n",
    "\n",
    "The solution is to expand beyond the single-layer architecture by adding another layer of units, known as the 'hidden layer'. This is known as a _multilayer perceptron (MLP)._ An MLP can have any number of hidden layers, and any number of units in each layer. They are fully connected, meaning all the nodes from the current layer are connected to the next. The presence of a hidden layer is exactly what allows the network to learn non-linear mappings; the first layer combines the inputs, and the second layer maps these combinations of inputs to the output.\n",
    "\n",
    "We can visualise the architecture thus:\n",
    "\n",
    "![Figure 2](img/multilayer.png \"Multilayer Perceptron\")\n",
    "\n",
    "\n",
    "The activation function used in MLPs is a nonlinear function - typically the __sigmoid function__ - which compresses the result to any continuous value between 0 and 1. Instead of performing a discrete binary classification, this therefore gives the probability of an input belonging to a certain class. \n",
    "\n",
    "The sigmoid function, which we will use in implementing the MLP, is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$σ(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And its __derivative__ - which we will need later - is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$σ'(z) = z(1-z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write the sigmoid function, along with its derivative, using the following code. We will also plot both functions to visualise their behaviour. Note how negative values are compressed toward 0, and positive values  to 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0025c78438>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define sigmoid function and its derivative \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "# Visualise the function\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.plot(z, sigmoid_prime(sigmoid(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation \n",
    "\n",
    "The first stage of a neural network algorithm is _forward propagation_. This is the process of generating an output prediction given the inputs. The prediction is arbitrary at first; it will be improved with training.\n",
    "\n",
    "\n",
    "An MLP performs multiple linear classifications in parallel, one for each of the C output classes. We therefore need a weight vector $(w_1,w_2,…,w_C)$ and a bias value $(b_1,b_2,…,b_C)$ for each class. The output $\\hat{y}$ of the perceptron should be a $C$-dimensional vector, whose entries contain the probability of the respective class:\n",
    "\n",
    "### $$\\begin{equation*}\n",
    "\\hat{y} = \n",
    "\\begin{pmatrix}\n",
    "σ(w_1^Tx + b_1) \\\\\n",
    "σ(w_2^Tx + b_2) \\\\\n",
    "... \\\\\n",
    "σ(w_C^Tx + b_C)\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}$$\n",
    "\n",
    "\n",
    "To do this efficiently, we can arrange the weight vectors in a matrix $W$, of dimensions ($d × C$), where $d$ is the dimensionality of the input dataset. We can also arrange the bias values in a vector $b$ of length $C$:\n",
    "\n",
    "### $W=(w_1,w_2,…,w_C)∈\\mathbb{R}^{d×C}$ \n",
    "\n",
    "\n",
    "### $\\vec{b}=(b_1,b_2,…,b_C)^T∈\\mathbb{R}^C$\n",
    "\n",
    "The __output__ is obtained by computing the dot product of the input matrix with the weight matrix $W$, adding the bias vector $\\vec{b}$, and applying the sigmoid function element-wise. \n",
    "\n",
    "The computation is therefore:\n",
    "\n",
    "### $$f(x) = σ (xW + \\vec{b})$$\n",
    "\n",
    "### Batch computation \n",
    "\n",
    "The matrix form allows to enter more than one data point at a time. Instead of a single point $x$, we can feed in a matrix $X∈\\mathbb{R}^{Nxd}$ containing one point per row. So there are $N$ rows of d-dimensional points. Instead of $xW$, we compute the matrix multiplication $XW$. This returns an $N × C$ matrix, with each row containing $xW$ for each data point $x$. To each row, we add a bias vector, which is now a $1 × m$ row vector. \n",
    "\n",
    "The forward propagation through the network is therefore essentially performing:\n",
    "\n",
    "### $$f(X) = σ (XW + \\vec{b})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation \n",
    "\n",
    "Before training, the network makes random predictions which are far from correct. Backpropagation is the method used to incrementally update the weights in order to train the network. First, the output from forward propagation is compared with the desired output value to obtain the _error_ in the prediction. We then use the optimisation method of _gradient descent_ to incrementally adjust all the weights in the network in the direction which reduces this error. The process is iterated until the predictions converge to the desired output. \n",
    "\n",
    "The process can be summarised in two stages:\n",
    "\n",
    "### Step 1.  Calculate the error \n",
    "\n",
    "We find the error or \"cost\" of the network's prediction by taking the difference between predicted and actual output. A common \"cost function\" used to do this is the _mean squared error (MSE)_. It is defined thus:\n",
    "\n",
    "### $$ MSE = \\frac{1}{n}\\sum_{i=1}^{n}(predicted - observed)^2 $$\n",
    "\n",
    "Where $n$ is the number of observations.\n",
    "\n",
    "### Step 2.   Minimise the cost\n",
    "\n",
    "Secondly, we need to minimise the cost function by finding the optimum combination of weights and biases across the network. To do this, we use the _gradient descent algorithm_. First, we calculate the partial deriative of the cost function with respect to each weight and bias. We then subtract the result from the current weights to get the updated weights at the next time step.\n",
    "\n",
    "To be explicit, let us denote the MSE cost function as $C$, and keep the desired output as $y$ and the predicted output as $\\hat{y}$. At each iteration, we then have:\n",
    "\n",
    "### $$ C = \\sum_{i=1}^{n}(y - \\hat{y})^2$$\n",
    "\n",
    "The __partial derivative__ of $C$ with respect to each weight $W$ can be expressed using the chain rule as:\n",
    "\n",
    "### $$\\frac{\\partial{C}}{\\partial{W}} = \\frac{\\partial{C}}{\\partial{\\hat{y}}} * \\frac{\\partial{\\hat{y}}}{\\partial{{z}}} * \\frac{\\partial{z}}{\\partial{W}}$$\n",
    "\n",
    "Here, $z$ is the activation $Wx + b$, before being activated by the sigmoid function.\n",
    "\n",
    "We know, analytically, all three terms of this equation:\n",
    "\n",
    "### $$\\frac{\\partial{C}}{\\partial{\\hat{y}}} = 2(y - \\hat{y}) \\;, \\;\\;\\;  \\frac{\\partial{\\hat{y}}}{\\partial{z}} = z(1-z) \\;, \\;\\;\\;  \\frac{\\partial{z}}{\\partial{W}} = x$$\n",
    "\n",
    " \n",
    "And thus can re-express as:\n",
    "\n",
    "### $$\\frac{\\partial{C}}{\\partial{W}} = 2(y - \\hat{y}) * z(1-z) * x$$\n",
    "\n",
    "This allows us to  calculate the __partial derivative of the loss function__ with respect to the weights explicitly. We then increment the weights by adding the dot product of the output of each layer with this partial derivative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an MLP to solve the XOR problem \n",
    "\n",
    "\n",
    "With this theoretical background in place, let us now code a simple MLP, with one hidden layer, capable of solving the XOR problem. We already have our activation function, input data and output data defined. The next step is to create the __weights and biases__. For this simple implementation, we will incorporate the biases into the weights as a __-1__ term. To make the code explicit at this stage, we will define the weights between the input-hidden and hidden-output layer as two separate variables, __W0__ and __W1.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed random numbers to make the output of the calculations deterministic \n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Obtain dimensions of input array\n",
    "\n",
    "l = len(x)           # Number of output classes, C\n",
    "l1 = len((x)[0])     # Dimensionality of input dataset, d\n",
    "\n",
    "# Initialise weight matrices W0 and W1\n",
    "\n",
    "W0 = 2*np.random.random((l1, l)) - 1  # dxC matrix of weights from input to hidden layer\n",
    "W1 = 2*np.random.random((l, 1)) - 1  # Cx1 matrix of weights from hidden to output layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us then create the __training loop__. First we perform forward propagation, then backpropagation. We run it over a large number of iterations (in this case, 50'000). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.4995289647766772\n",
      "Error: 0.018951601482048636\n",
      "Error: 0.012674637262799143\n",
      "Error: 0.010027680242236311\n",
      "Error: 0.00850592237103725\n"
     ]
    }
   ],
   "source": [
    "# Set the number of iterations to perform\n",
    "\n",
    "iterations = 50000\n",
    "\n",
    "# Create the training loop \n",
    "\n",
    "for j in range(iterations):\n",
    "    l0 = x                            # Set the first layer, l0, equal to the training data \n",
    "    l1 = sigmoid(np.dot(l0, W0))      # Set the hidden layer, l1, equal to the result of forward propagation on the first layer\n",
    "    l2 = sigmoid(np.dot(l1, W1))      # Set the output layer, l2, equal to the result of forward propagation on the second layer\n",
    "    \n",
    "    # Calculate the error between the predicted and desired output\n",
    "    \n",
    "    l2_error = y - l2\n",
    "    \n",
    "    # Print the MSE every 10000 iterations, to visualise the improvement over time\n",
    "    \n",
    "    if j%10000 == 0:\n",
    "        MSE = np.mean(np.abs(l2_error))\n",
    "        print(\"Error: \" + str(MSE))\n",
    "        \n",
    "    # Backpropagate the error through l2 and l1 to obtain how each l1 output contributes to the l2 error \n",
    "    \n",
    "    l2_delta = l2_error * sigmoid_prime(l2) \n",
    "    l1_error = l2_delta.dot(W1.T)\n",
    "    l1_delta = l1_error * sigmoid_prime(l1)    # Produces a greater value for l1 values which contribute more greatly to the error \n",
    "    \n",
    "    # Update the weights in a manner which aims to counteract the error they produce \n",
    "    \n",
    "    W1 += np.dot(l1.T, l2_delta)    # If the weights contribute a lot to the error in their layer, the update is a large in the opposite direction to the error, and vice versa\n",
    "    W0 += np.dot(l0.T, l1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the error decreasing progressively over the iterations. Let's now print the network's output prediction after 50'000 iterations. We can print the desired output, y, as a reminder of what we're aiming for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired output: \n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "Output after training:\n",
      "[[0.00886467]\n",
      " [0.99345174]\n",
      " [0.9920439 ]\n",
      " [0.00662409]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Desired output: \")\n",
    "print(y)\n",
    "\n",
    "print(\"\\n\" + \"Output after training:\")\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the network's predictions have converged to be very close to our desired outputs. In other words, the MLP has learned to correctly classify the inputs in the truth table of the XOR problem. Indeed, if we round the outputs to their nearest integer, we can see this more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded output after training: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Rounded output after training: \")\n",
    "print(np.rint(l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "\n",
    "To conclude, I would like to summarise what this project has achieved. After a brief introduction to the __XOR problem,__ we have explored the theoretical background behind the computational architecture capable of solving it, the __multilayer perceptron__ - as well as its precursor, the more simple __perceptron.__ After exploring the basic mathematical principles behind __forward propagation__ and __backpropagation,__ we then trained a simple 3-layer MLP architecture on the classification problem of XOR. We have shown that the network has can achieve an adequate classification of the inputs after 50'000 training iterations. We have also visualised the reduction in error over time. Using this code, we can experiment with various parameters such as the weight initialisations and number of training iterations, to explore the nature of how the network behaves. It would be interesting to extend the implementation to include separate __bias vectors,__ an incremental weight update via a __learning rate__, or perhaps a different nonlinear activation function such as a __ReLu.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "To wrap up, some words about my personal experiences with this project. Although it may not appear as such, this endeavour was a huge undertaking for me. I spent an enormous amount of time immersing myself in the hype of neural networks, consuming resource after resource, following what appeared to be a never-ending chain of potential information. Since the field has exploded so much in recent years, there is an enormous amount of content online - web tutorials, books, mini-series, video tutorials, online courses - each proposing their own angle of attack. This was both a blessing and a curse for me. It was extremely useful to have such a wealth of perpectives, and such scope of examples to learn from - but it was also extremely overwhelming to navigate, and left me always burrowing down rabbit holes that were difficult to emerge from. I would spend days reading different angles of explanation from different disciplines, and experimenting with architectures that were all supposed to be doing the same thing, but didn't match. Ultimately, I have still not been able to fully synthesise the common ground between them. This is why I ended up limiting myself to the overly-simplistic result we find here, instead of 'feigning' a mastery of what to me is still unclear. My small fear is that such a result is not reflective, and does not fully 'do justice' to the amount of work that was involved in reaching this point - but this should hardly be a matter of concern.\n",
    "\n",
    "Ultimately, I am extremely happy to have carried out this project, and am grateful to PCBS for the excuse to spend time on something I have been wishing to explore deeply. I enjoyed immensely the process of discovery, immersion, and sometimes complete 'burial' in something which has for a long time been a black box. I especially enjoyed getting to grips with the calculus of backpropagation and spending hours getting lost in the dimensionality of different matrices. Most pages of my notebooks are now covered in some form of lines and dots. But, I shall be grateful, for my basic linear algebra intuitions have greatly improved as a result - something which is personally crucial from here on out. I also hugely enjoyed learning the basics of LaTeX for the purposes of the Jupyter Notebook write-up, which I have been wishing to do for some time. Now I am extremely motivated to improve my efficiency in both LaTeX and markdown, recognising how useful and streamlined they can be. Finally, I look forward to making slow but sure progress in Python - and continuing to be motivated, and hopefully not paralysed, by the recognition of how much I do not know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
