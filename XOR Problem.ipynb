{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction ###\n",
    "\n",
    "A neural network is a method in the field of machine learning used to build predictive models to help solve complex tasks. The network is first exposed to a large amount of data, then the system is trained to learn the best predictions by itself.\n",
    "\n",
    "The components of neural networks are 'neurons' or nodes, which take in any number of inputs, apply a mathematical function to them, and return outputs onward to other neurons. Each input is weighted; the weights are free parameters to be adjusted during training. The mathematical function is known as the 'activation function'.\n",
    "\n",
    "The network consists of many layers of neurons; the outputs of one layer are inputs to the next. By stacking layers together, the basic activation functions can form more complex mathematical functions. The aim is to construct a function which maps the inputs to the desired outputs. This is done by adjusting the weights each neuron applies to its inputs.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The XOR, or “exclusive or”, problem is the problem of using a neural network to predict the outputs of XOR logic gates given two binary inputs. An XOR function should return a true value if the two inputs are not equal and a false value if they are equal. All possible inputs and predicted outputs are shown in figure 1.\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "XOR is a classification problem. Expected outputs are known in advance. So we use suprvised learning. \n",
    "\n",
    "On the surface, XOR appears to be a simple problem. However, Minsky and Papert (1969) explained how it was a huge problem for neural network architectures at the time (perceptrons).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input array (x) and output array (y)\n",
    "\n",
    "x = np.array([[0, 0], \n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]]) \n",
    "\n",
    "y = np.array([[0], \n",
    "             [1], \n",
    "             [1],\n",
    "             [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrons ###\n",
    "\n",
    "Perceptrons have a single layer of input units. The input units X1, X2 etc. are multiplied by their respective weights W0, W1 etc., and passed to the output unit. A bias is added. This is then passed through an activation function (typically Heavside step function), converting the resulting value to a 0 or 1. \n",
    "\n",
    "It is the weight variables that controls the conversion of input to output. And biases. \n",
    "\n",
    "Single-layer perceptrons are only capable of separating data points with a single line. XOR inputs are not linearly separable. There is no way to separate the 1 and 0 predictions with a single line. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise classfication problem - note nonlinearity \n",
    "\n",
    "plt.grid()\n",
    "\n",
    "for i in range(len(x)):\n",
    "    c = 'r'\n",
    "    if y[i] == 0:\n",
    "        c = 'b'\n",
    "    plt.scatter([x[i][0]], [x[i][1]], c=c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptrons ###\n",
    "\n",
    "The solution is to expand beyond the single-layer architecture by adding another layer of units, known as the 'hidden layer'. This is known as a multilayer perceptron (MLP). \n",
    "\n",
    "An MLP can have any number of units in its input, hidden and output layers, and any number of hidden layers.\n",
    "\n",
    "Here, forward propagation: input values multiplied with respective weights, bias added, passed through activation function (typically sigmoid function). The outputs of each hidden layer unit are then multiplied by another set of weights and parsed to an output unit. After being passed through another activation function, the network outputs its prediction (values between 0 and 1). \n",
    "\n",
    "This architecture is capable of achieving nonlinear separation. Thus, it can correctly classify XOR inputs. \n",
    "\n",
    "The Universal Approximation Theorem states that a 2-layer network can approximate any function, given a complex enough architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation ###\n",
    "\n",
    "How do we know the correct weight values?\n",
    "\n",
    "Trying to find out manually would be incredibly laborious (and is NP-complete). Can learn automatically a good approximation through backpropagation. First demonstrated for XOR problem by Rumelhart et al. (1985).\n",
    "\n",
    "First - compare predicted value output by forward propagation to the actual value (y). Get 'error'. Adjust the weights in a direction that reduces the size of the error. Both forward and backpropagation are run thousands of times on each input combination until the weights are well-adjusted.\n",
    "\n",
    "BP is a way to 'intelligently train' the network. \n",
    "\n",
    "**Loss function** - a way to quantify the performance of the network. The difference between the network's predictions and actual outputs. E.g. mean-squared error, cross-entropy loss. \n",
    "\n",
    "Training becomes an optimisation problem of minimising the loss function. Although advanced techniques exist - e.g. simulated annealing, genetic algorithms - to find global minima of black-box functions, the huge number of free parameters in these networks means these approaches are very slow to converge. \n",
    "\n",
    "Though the loss function contains many local minima, each minima is about as optimal as any other. We can use the **gradient descent** algorithm to find such minima. Gradient descent calculates the slope fo the loss function at the current point in parameter space and moves in the direction in which the gradient is steepest. \n",
    "\n",
    "**Updating the network**\n",
    "\n",
    "We take a data point and do a *forward pass* through the network.\n",
    "We then evaluate the loss function of the output.\n",
    "We do a *backward pass* thgouh the network, finding the gradient of the loss function. Backpropagation allows to analytically derive the effect of each parameter in each layer on the output of the network. We adjust each parameter by taking a small step *down* the gradient. The size of the step is referred to as the learning rate, eta.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation function and its derivative \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return z*(1-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdda8d49fd0>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise the function\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.plot(z, sigmoid_prime(sigmoid(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights, biases and hyperparameters\n",
    "\n",
    "\n",
    "l = len(x)           # Get dimensions of input array \n",
    "l1 = len((x)[0])\n",
    "\n",
    "np.random.seed(0)     # Set random seed\n",
    "\n",
    "# Set weights\n",
    "wi = np.random.random((l1, l))   # Weights from input to hidden layer\n",
    "wh = np.random.random((l, 1))    # Weights from hidden to output layer\n",
    "\n",
    "# Set biases \n",
    "bi = np.random.random((1, l))    # Biases from input to hidden layer\n",
    "bh = np.random.random((1, 1))    # Biases from hidden to output layer\n",
    "\n",
    "epochs = 100000       # Set number of epochs (training iterations)\n",
    "eta = 1               # Set learning rate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final hidden weights: \n",
      " [[7.12415454 1.53650714 5.67917875 5.22985046]\n",
      " [7.11503214 1.53195597 5.68137635 5.22446067]]\n",
      "\n",
      " Final hidden bias: \n",
      " [[-3.27900243 -2.25280785 -8.61446641 -2.30582472]]\n",
      "\n",
      " Final output weights: \n",
      " [[ 11.18269885]\n",
      " [ -5.11639206]\n",
      " [-13.99667467]\n",
      " [  5.83389664]]\n",
      "\n",
      " Final output bias: \n",
      " [[-7.16586739]]\n",
      "\n",
      " Output from neural network after 100000 epochs: \n",
      " [[0.00120472]\n",
      " [0.9990269 ]\n",
      " [0.99902687]\n",
      " [0.0010573 ]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    \n",
    "    # Forward pass\n",
    "    \n",
    "    a0 = x                                # Activation of input layer\n",
    "    a1 = sigmoid(np.dot(a0, wi) + bi)     # Activation of hidden layer\n",
    "    a2 = sigmoid(np.dot(a1, wh) + bh)     # Activation of output layer\n",
    "    \n",
    "    # Backpropagation\n",
    "    \n",
    "    a2_err = y - a2           # Error = actual output - predicted outut\n",
    "    a2_delta = np.multiply(a2_err, sigmoid_prime(a2))\n",
    "    \n",
    "    a1_err = np.dot(a2_delta, wh.T)\n",
    "    a1_delta = np.multiply(a1_err, sigmoid_prime(a1))\n",
    "    \n",
    "    # Update weights and biases\n",
    "    \n",
    "    wh += eta * (np.dot(a1.T, a2_delta))\n",
    "    wi += eta * (np.dot(a0.T, a1_delta))\n",
    "    \n",
    "    bh += eta * np.sum(a2_delta, axis=0, keepdims=True)\n",
    "    bi += eta * np.sum(a1_delta, axis=0, keepdims=True)\n",
    "    \n",
    "# Print final parameters\n",
    "    \n",
    "print('Final hidden weights: \\n %s' % wi)\n",
    "print('\\n Final hidden bias: \\n %s' % bi)\n",
    "print('\\n Final output weights: \\n %s' % wh)\n",
    "print('\\n Final output bias: \\n %s' % bh)\n",
    "\n",
    "# Print final predictions\n",
    "\n",
    "print('\\n Output from neural network after %s epochs: \\n %s' % (epochs, a2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with sigmoidal activation function ###\n",
    "\n",
    "The gradient converges asymptotically to zero at large absolute values of x. So if the activation of the neuron is not in the central region, the local gradient is zero. During backprop, incoming gradient gets multiplied by this local gradient, leading to 'vanishing gradients'. The neurons can thereby not update their weights, since the updates depend on the gradients. This can lead to 'dead neurons' within the network. \n",
    "\n",
    "The second problem is that the outputs are not zero-centred. The gradient is either always positive or always negative. If the optimum set of weights happens to be a mixture of positive and negative weights, this can only be reached by inefficient zigzag updates.\n",
    "\n",
    "A solution is to use a rectified linear unit (ReLU). Here the gradient never saturates in the positive region, and is quick to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with initialisation ###\n",
    "\n",
    "Default approach to initialising weights is to sample them from Gaussian distribution. If they are too low, the output of the neurons is too small and the gradient propagated to the weights vanishes. \n",
    "\n",
    "In 2010, Bengio & Glorot derived a new initialisations cheme - Xavier initalisation. Sets the factor equal to one over the square root of th enumber of inputs to a neuron. So neurons with many inputs have lower weights, and vice versa. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with convergence time ###\n",
    "\n",
    "Gradient descent can get stuck in gently sloping valleys. The updates cause zigzag oscillations between the sides of the valley, while the optimal path would be to move along teh centre.\n",
    "\n",
    "Add a momentum step.\n",
    "\n",
    "More improvements  https://medium.com/series/839cfe45cac9 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop - more detail ###\n",
    "\n",
    "Principle of backprop - to model a given function by modifying internal weightings of input signals to produce expected output signal. Trained with supervised leadning method, where error between system's output and known expected output is presented to system and used to modify internal state.\n",
    "\n",
    "Backprop is method for training weights of multilayer feedforward neural network. Works for both regression + classification.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
